{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectada.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A21m2oXXf5uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEYyWedtgEnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL = 'https://marvel.fandom.com/wiki/Category:Earth-616_Characters'\n",
        "r = requests.get(URL)\n",
        "page_body = r.text\n",
        "soup = BeautifulSoup(page_body, 'html.parser')\n",
        "\n",
        "#collect url of each character in the web page\n",
        "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
        "list_url = []\n",
        "for p in publications_wrappers:\n",
        "  for a in p.find_all('a', href=True):\n",
        "    list_url.append(a['href'])\n",
        "#remove duplicate\n",
        "my_set =set(list_url)\n",
        "good_list_url = list(my_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZfGvEaL77bQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_columns = ['Real Name', 'Current Alias', 'Identity','Citizenship',\n",
        "                'Marital Status', 'Occupation',\n",
        "                'Education', 'Gender', 'Height', 'Weight',\n",
        "                'Eyes','Hair',\n",
        "                'Place of Birth']\n",
        "personnage_pd = pd.DataFrame(columns=new_columns)\n",
        "idx =0\n",
        "dict_geant={}\n",
        "for pers in good_list_url:\n",
        "  # Get URL and use html parser\n",
        "  URL_char = 'https://marvel.fandom.com' + pers\n",
        "  URL_char = URL_char.replace(\"'\",\"\")\n",
        "  r_char = requests.get(URL_char)\n",
        "  page_body_char = r_char.text\n",
        "  soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
        "  # Initialize variables\n",
        "  name, identity, current_alias, citizenship, marital_status, occupation = '','','','','',''\n",
        "  education, gender, height, weight, eyes, hair, place_of_birth = '','','','','','',''\n",
        "  personnage =[]\n",
        "  # Parsing\n",
        "  side_tab = soup_char.find_all('div', class_='conjoined-infoboxes')\n",
        "  for p in side_tab:\n",
        "    for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
        "        for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
        "            if(ppp.text[1:]==\"Real Name\"):\n",
        "                name = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Identity\"):\n",
        "                identity = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Current Alias\"):\n",
        "                current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Citizenship\"):\n",
        "                citizenship = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Marital Status\"):\n",
        "                marital_status = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Occupation\"):\n",
        "                occupation = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Education\"):\n",
        "                education = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Gender\"):\n",
        "                gender = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Height\"):\n",
        "                height = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Weight\"):\n",
        "                weight = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Eyes\"):\n",
        "                eyes = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Hair\"):\n",
        "                hair = pp.find('div', class_='pi-data-value pi-font').text\n",
        "            if(ppp.text[1:]==\"Place of Birth\"):\n",
        "                place_of_birth = pp.find('div', class_='pi-data-value pi-font').text\n",
        "\n",
        "  characteristics_pd = pd.DataFrame([[name[1:], identity, current_alias, citizenship, marital_status, occupation,\n",
        "                     education, gender, height, weight, eyes, hair, place_of_birth]], columns = new_columns)\n",
        "  personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
        "personnage_pd.head()\n",
        "nextpage = soup.find('link', {\"rel\" : \"next\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiA6X_0ongme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9luW0bDA-Cn",
        "colab_type": "code",
        "outputId": "6c3b69e7-47f3-48c5-d05a-65e036639cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "while(len(nextpage['href'])):\n",
        "  urlnext_page = nextpage['href']\n",
        "  r = requests.get(urlnext_page)\n",
        "  page_body = r.text\n",
        "  soup = BeautifulSoup(page_body, 'html.parser')\n",
        "\n",
        "  #collect url of each character in the web page\n",
        "  publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
        "  list_url = []\n",
        "  for p in publications_wrappers:\n",
        "    for a in p.find_all('a', href=True):\n",
        "      list_url.append(a['href'])\n",
        "  #remove duplicate\n",
        "  my_set =set(list_url)\n",
        "  good_list_url = list(my_set)\n",
        "  idx =0\n",
        "  dict_geant={}\n",
        "  for pers in good_list_url:\n",
        "    # Get URL and use html parser\n",
        "    URL_char = 'https://marvel.fandom.com' + pers\n",
        "    URL_char = URL_char.replace(\"'\",\"\")\n",
        "    r_char = requests.get(URL_char)\n",
        "    page_body_char = r_char.text\n",
        "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
        "    # Initialize variables\n",
        "    name, identity, current_alias, citizenship, marital_status, occupation = '','','','','',''\n",
        "    education, gender, height, weight, eyes, hair, place_of_birth = '','','','','','',''\n",
        "    personnage =[]\n",
        "    # Parsing\n",
        "    side_tab = soup_char.find_all('div', class_='conjoined-infoboxes')\n",
        "    for p in side_tab:\n",
        "      for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
        "          for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
        "              if(ppp.text[1:]==\"Real Name\"):\n",
        "                  name = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Identity\"):\n",
        "                  identity = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Current Alias\"):\n",
        "                  current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Citizenship\"):\n",
        "                  citizenship = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Marital Status\"):\n",
        "                  marital_status = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Occupation\"):\n",
        "                  occupation = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Education\"):\n",
        "                  education = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Gender\"):\n",
        "                  gender = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Height\"):\n",
        "                  height = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Weight\"):\n",
        "                  weight = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Eyes\"):\n",
        "                  eyes = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Hair\"):\n",
        "                  hair = pp.find('div', class_='pi-data-value pi-font').text\n",
        "              if(ppp.text[1:]==\"Place of Birth\"):\n",
        "                  place_of_birth = pp.find('div', class_='pi-data-value pi-font').text\n",
        "\n",
        "    characteristics_pd = pd.DataFrame([[name[1:], identity, current_alias, citizenship, marital_status, occupation,\n",
        "                      education, gender, height, weight, eyes, hair, place_of_birth]], columns = new_columns)\n",
        "    personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
        "    nextpage = soup.find('link', {\"rel\" : \"next\"})\n",
        "\n",
        "personnage_pd.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-97cdf2c233c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextpage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0murlnext_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnextpage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlnext_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpage_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q4aqzJGE7jP",
        "colab_type": "code",
        "outputId": "c06ab718-59d9-402d-fb0a-dc4c42be92cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(personnage_pd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28018, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKsIds0zH66z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nextpage = soup.find('link', {\"rel\" : \"next\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdnotqGsJ-tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def local_persist(frout, results):\n",
        "  frout = frout + '.txt'\n",
        "  pickle.dump(results, open(frout, 'wb'))\n",
        "  files.download(frout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL8UmTkNKAgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "from google.colab import files\n",
        "\n",
        "personnage_pd.to_csv('personnage_pd.csv')\n",
        "files.download('personnage_pd.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYjKCWmJHGD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(personnage_pd, open('save_pickle.txt', 'wb'))\n",
        "files.download('save_pickle.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3135zWOIPVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}